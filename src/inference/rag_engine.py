import os
import sys
import torch
from threading import Thread
from collections import deque
from typing import Optional, Dict

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from sentence_transformers import CrossEncoder 

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig, 
    TextIteratorStreamer
)

from config import (
    DB_PATH, 
    LLM_MODEL_ID, 
    EMBEDDING_MODEL_ID, 
    RERANKER_MODEL_ID, # [New] configì—ì„œ ê°€ì ¸ì˜¤ê¸°
    DEVICE, 
    MAX_NEW_TOKENS, 
    TEMPERATURE
)

class RAGEngine:
    def __init__(self):
        # Configì˜ DEVICE ì‚¬ìš©
        self.device = DEVICE
        print(f"[Init] Device: {self.device}")
        
        self._load_vector_db()
        self._load_reranker() 
        self._load_llm()
        self.chat_history = deque(maxlen=3)

    def _load_vector_db(self):
        if not os.path.exists(DB_PATH):
            raise FileNotFoundError(f"DB Not Found at {DB_PATH}")

        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_ID,
            model_kwargs={'device': self.device},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        self.vector_store = Chroma(
            persist_directory=DB_PATH,
            embedding_function=self.embeddings,
            collection_name="samsung_report_db"
        )

    def _load_reranker(self):
        """[New] Cross-Encoder Reranker ë¡œë“œ"""
        print(f"[Init] Reranker ë¡œë”© ({RERANKER_MODEL_ID})...")
        # automodel_argsë¥¼ ì‚¬ìš©í•˜ì—¬ torch_dtype ì„¤ì •
        self.reranker = CrossEncoder(
            RERANKER_MODEL_ID, 
            device=self.device,
            automodel_args={"torch_dtype": "auto"}
        )

    def _load_llm(self):
        print(f"[Init] LLM ë¡œë”© ({LLM_MODEL_ID})...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )
        self.tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)
        self.model = AutoModelForCausalLM.from_pretrained(
            LLM_MODEL_ID,
            quantization_config=bnb_config,
            device_map="auto"
        )
        self.terminators = [
            self.tokenizer.eos_token_id,
            self.tokenizer.convert_tokens_to_ids("<|eot_id|>"),
            self.tokenizer.convert_tokens_to_ids("<|end_of_text|>")
        ]

    def search(self, query: str, filters: Optional[Dict] = None, k: int = 3):
        """
        [Upgrade] 2ë‹¨ê³„ ê²€ìƒ‰ ì‹œìŠ¤í…œ
        1. Vector Searchë¡œ í›„ë³´êµ°(3ë°°ìˆ˜) ì¶”ì¶œ
        2. Cross-Encoderë¡œ ì •ë°€ ì±„ì (Reranking) í›„ Top-k ë°˜í™˜
        """
        # 1. ì´ˆê¸° í›„ë³´êµ° ê²€ìƒ‰ (ìµœì¢… kì˜ 3ë°°ìˆ˜ ì •ë„ ê°€ì ¸ì˜´)
        initial_k = k * 3 
        
        search_kwargs = {"k": initial_k}
        if filters:
            conditions = [{key: {"$eq": val}} for key, val in filters.items()]
            if len(conditions) > 1:
                search_kwargs["filter"] = {"$and": conditions}
            elif len(conditions) == 1:
                search_kwargs["filter"] = conditions[0]

        # ChromaDBì—ì„œ 1ì°¨ ê²€ìƒ‰
        docs = self.vector_store.similarity_search(query, **search_kwargs)
        
        if not docs:
            return []

        # 2. [Reranking] ì •ë°€ ì±„ì 
        # (ì§ˆë¬¸, ë¬¸ì„œë‚´ìš©) ìŒì„ ìƒì„±
        pairs = [[query, doc.page_content] for doc in docs]
        
        # CrossEncoderê°€ ë¬¸ë§¥ ì—°ê´€ì„± ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        scores = self.reranker.predict(pairs)

        # 3. ì ìˆ˜ì™€ ë¬¸ì„œ ê²°í•© ë° ì •ë ¬
        scored_docs = []
        for doc, score in zip(docs, scores):
            doc.metadata["rerank_score"] = float(score) # ë©”íƒ€ë°ì´í„°ì— ì ìˆ˜ ê¸°ë¡ (ë””ë²„ê¹…ìš©)
            scored_docs.append(doc)

        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        scored_docs.sort(key=lambda x: x.metadata["rerank_score"], reverse=True)

        # ìƒìœ„ kê°œë§Œ ì„ íƒ
        final_docs = scored_docs[:k]
        
        # (ì˜µì…˜) ë¡œê·¸ ì¶œë ¥
        if final_docs:
            print(f"[Search] Top score: {final_docs[0].metadata['rerank_score']:.4f}")

        return final_docs

    def chat(self, query: str, filters: Optional[Dict] = None):
        """Generator ë°©ì‹ìœ¼ë¡œ UIì— ìŠ¤íŠ¸ë¦¬ë°"""
        
        # 1. Retrieve (Rerankerê°€ ì ìš©ëœ search í˜¸ì¶œ)
        docs = self.search(query, filters, k=3)
        
        context_parts = []
        sources = []
        for doc in docs:
            meta = doc.metadata
            src = f"{meta.get('company', 'Unknown')} {meta.get('year', '')}"
            page = meta.get('page', '?') # í˜ì´ì§€ ì •ë³´ê°€ ìˆë‹¤ë©´ í‘œì‹œ
            
            # ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½
            context_parts.append(f"[{src} p.{page}]\n{doc.page_content.strip()}")
            
            # ì¶œì²˜ ëª©ë¡ ì¡°ë¦½
            filename = os.path.basename(meta.get('source', 'Unknown'))
            sources.append(f"- ğŸ“„ **{filename}** (p.{page})")

        context_text = "\n\n".join(context_parts)

        # 2. Prompt Setup
        system_prompt = (
            "ë‹¹ì‹ ì€ ê¸°ì—… ë³´ê³ ì„œ ë¶„ì„ AIì…ë‹ˆë‹¤. [ì°¸ê³  ë¬¸ì„œ]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. "
            "ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ë‚´ì§€ ë§ê³ , ìˆ˜ì¹˜ì™€ ì‚¬ì‹¤ ìœ„ì£¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”."
        )

        messages = [{"role": "system", "content": system_prompt}]
        for old_q, old_a in self.chat_history:
            messages.append({"role": "user", "content": old_q})
            messages.append({"role": "assistant", "content": old_a})
        messages.append({"role": "user", "content": f"[ì°¸ê³  ë¬¸ì„œ]\n{context_text}\n\nì§ˆë¬¸: {query}"})

        # 3. Generation Setup
        input_ids = self.tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(self.device)

        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        gen_kwargs = dict(
            input_ids=input_ids,
            streamer=streamer,
            max_new_tokens=MAX_NEW_TOKENS,
            temperature=TEMPERATURE,
            repetition_penalty=1.15,
            do_sample=True,
            eos_token_id=self.terminators
        )

        thread = Thread(target=self.model.generate, kwargs=gen_kwargs)
        thread.start()

        # 4. Yield Stream
        full_response = ""
        for new_text in streamer:
            if any(k in new_text for k in ["ì§ˆë¬¸:", "User:"]): break
            full_response += new_text
            yield new_text

        # 5. Sources
        if sources:
            source_footer = "\n\n**[ì°¸ê³  ë¬¸ì„œ]**\n" + "\n".join(sorted(list(set(sources)))) # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            yield source_footer
            full_response += source_footer
        
        self.chat_history.append((query, full_response))