{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e22945f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# ==========================================\n",
    "# [설정] 한글 폰트 설정 (WSL/Linux 환경)\n",
    "# ==========================================\n",
    "# 나눔고딕이 설치되어 있다고 가정 (/usr/share/fonts/truetype/nanum/NanumGothic.ttf)\n",
    "# 폰트가 없다면 기본 폰트로 실행되지만 한글이 깨질 수 있습니다.\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ==========================================\n",
    "# [1] 데이터 로드 및 길이 분석\n",
    "# ==========================================\n",
    "DATA_DIR = \"data/processed/sections\"\n",
    "files = glob.glob(os.path.join(DATA_DIR, \"*.md\"))\n",
    "\n",
    "print(f\"총 파일 개수: {len(files)}\")\n",
    "\n",
    "doc_lengths = []\n",
    "all_text = \"\"\n",
    "\n",
    "for file_path in files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        doc_lengths.append(len(content))\n",
    "        all_text += content + \" \"\n",
    "\n",
    "# 통계 출력\n",
    "print(f\"\\n=== 텍스트 길이 통계 (글자 수) ===\")\n",
    "print(f\"최소 길이: {min(doc_lengths)}\")\n",
    "print(f\"최대 길이: {max(doc_lengths)}\")\n",
    "print(f\"평균 길이: {sum(doc_lengths) / len(doc_lengths):.2f}\")\n",
    "\n",
    "# 히스토그램 그리기\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(doc_lengths, bins=30, kde=True)\n",
    "plt.title(\"Distribution of Section Lengths (Characters)\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show() # 혹은 plt.savefig('eda_length_dist.png')\n",
    "\n",
    "# ==========================================\n",
    "# [2] 청크(Chunk) 시뮬레이션\n",
    "# ==========================================\n",
    "# 실제 RAG에서 사용할 청크 크기가 적절한지 미리 테스트해봅니다.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "# 전체 텍스트를 청킹해보며 분포 확인\n",
    "sample_chunks = splitter.split_text(all_text[:100000]) # 너무 많으면 샘플링\n",
    "chunk_lengths = [len(c) for c in sample_chunks]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(chunk_lengths, bins=30, color='orange', kde=True)\n",
    "plt.title(f\"Chunk Length Distribution (Size={CHUNK_SIZE}, Overlap={CHUNK_OVERLAP})\")\n",
    "plt.xlabel(\"Chunk Length (Characters)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.axvline(CHUNK_SIZE, color='r', linestyle='--', label='Target Size')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# [3] 키워드 빈도 분석 (간단 버전)\n",
    "# ==========================================\n",
    "# 공백 기준으로 단어 분리 (정교한 분석을 위해선 Konlpy 사용 권장)\n",
    "words = all_text.split()\n",
    "# 2글자 이상인 단어만 필터링\n",
    "words = [w for w in words if len(w) >= 2]\n",
    "\n",
    "counter = Counter(words)\n",
    "top_20 = counter.most_common(20)\n",
    "\n",
    "print(\"\\n=== 상위 20개 빈출 단어 ===\")\n",
    "for word, count in top_20:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# 막대 그래프\n",
    "top_words, top_counts = zip(*top_20)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(top_counts), y=list(top_words))\n",
    "plt.title(\"Top 20 Frequent Words\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
