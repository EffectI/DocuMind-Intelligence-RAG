import os
import torch
from threading import Thread
from collections import deque

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig, 
    TextIteratorStreamer
)

# ==========================================
# [ì„¤ì •] ê²½ë¡œ ë° ëª¨ë¸
# ==========================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, "data", "vector_db")

LLM_MODEL_ID = "beomi/Llama-3-Open-Ko-8B"
EMBEDDING_MODEL = "BAAI/bge-m3"

def main():
    # 1. GPU ì„¤ì •
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[1] System Check: {device}")
    
    # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print("Loading Embeddings...")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': device}
    )

    # 3. ë²¡í„° DB ë¡œë“œ
    if not os.path.exists(DB_PATH):
        print("Error: DB Not Found. Run simple_rag.py first.")
        return

    vector_store = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name="samsung_report_db"
    )
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    # 4. LLM ë¡œë“œ
    print(f"Loading LLM ({LLM_MODEL_ID})...")
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)
    model = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL_ID,
        quantization_config=bnb_config, 
        device_map="auto",
    )

    # =========================================================
    # ëŒ€í™” ê¸°ë¡ ì €ì¥ì†Œ
    # =========================================================
    chat_history = deque(maxlen=5) 

    # =========================================================
    # ìŠ¤íŠ¸ë¦¬ë° í•¨ìˆ˜
    # =========================================================
    def stream_response(query, history):
        print(f"\nQuestion: {query}")
        
        # 1. ë¬¸ì„œ ê²€ìƒ‰
        docs = retriever.invoke(query)

        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ë¡œê·¸ ì¶œë ¥
        print("\n" + "="*60)
        print(f"ğŸ” [DEBUG] Retrieverê°€ ì°¾ì•„ì˜¨ ë¬¸ì„œ ({len(docs)}ê°œ)")
        print("="*60)
        for i, doc in enumerate(docs):
            source = doc.metadata.get('source', 'Unknown Source')
            page = doc.metadata.get('page', 'Unknown Page')
            content = doc.page_content.strip()
            
            print(f"[Chunk {i+1}] (Source: {source}, Page: {page})")
            print("-" * 30)
            print(content) # ë¬¸ì„œ ë‚´ìš© ì „ì²´ ì¶œë ¥
            print("-" * 60)
        print("============================================================\n")

        context_text = "\n\n".join([d.page_content for d in docs])
        
        print("AI Answer: ", end="", flush=True)

        # [ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìœ ì§€]
        system_prompt = (
            "ë‹¹ì‹ ì€ ì‚¼ì„±ì „ì ì‚¬ì—…ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. "
            "ì œê³µëœ [ì°¸ê³  ë¬¸ì„œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. "
            "ë‹µë³€ì´ ëë‚˜ë©´ ë¶ˆí•„ìš”í•œ ë¶€ì—° ì„¤ëª…ì´ë‚˜ ë‹¨ìœ„ ë³€í™˜ ëª©ë¡(ì˜ˆ: 1GB=...)ì„ ì ˆëŒ€ ì‘ì„±í•˜ì§€ ë§ê³  ì¦‰ì‹œ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ì„¸ìš”."
        )

        # 2. ë©”ì‹œì§€ êµ¬ì¡° ìƒì„±
        messages = [{"role": "system", "content": system_prompt}]

        for old_query, old_answer in history:
            messages.append({"role": "user", "content": old_query})
            messages.append({"role": "assistant", "content": old_answer})

        messages.append({
            "role": "user", 
            "content": f"[ì°¸ê³  ë¬¸ì„œ]\n{context_text}\n\nì§ˆë¬¸: {query}"
        })
        
        # 3. í† í°í™”
        input_ids = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(model.device)

        # 4. ìŠ¤íŠ¸ë¦¬ë¨¸ ì¤€ë¹„
        
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        # [ì¢…ë£Œ í† í° ì„¤ì • ìœ ì§€]
        terminators = [
            tokenizer.eos_token_id,
            tokenizer.convert_tokens_to_ids("<|eot_id|>"),
            tokenizer.convert_tokens_to_ids("<|end_of_text|>")
        ]

        # 5. ìƒì„± ì‹œì‘
        generation_kwargs = dict(
            input_ids=input_ids, 
            streamer=streamer, 
            max_new_tokens=512, 
            temperature=0.09,        
            repetition_penalty=1.15, 
            do_sample=True,        
            eos_token_id=terminators 
        )
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()

        # 6. ì‹¤ì‹œê°„ ì¶œë ¥ ë° í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        generated_text = ""
        
        stop_keywords = ["ì§ˆë¬¸:", "User:", "Question:", "ì‚¬ìš©ì:", "<|eot_id|>", "<|end_of_text|>"]

        for new_text in streamer:
            should_stop = False
            for keyword in stop_keywords:
                if keyword in new_text:
                    should_stop = True
                    break
            
            if should_stop:
                break

            print(new_text, end="", flush=True)
            generated_text += new_text
            
        print("\n")
        return generated_text

    # 7. ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
    print("\n[2] RAG Chatbot Started (Type 'q' to exit)")
    
    while True:
        try:
            query = input("\nInput: ")
            if query.lower() in ['q', 'quit', 'exit']:
                break
            if not query.strip():
                continue
                
            response_text = stream_response(query, chat_history)
            chat_history.append((query, response_text))
            
        except KeyboardInterrupt:
            break

if __name__ == "__main__":
    main()